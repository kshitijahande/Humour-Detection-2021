{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf20f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729d7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the name of emotion_module as it is conflicting\n",
    "# with weight loading\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.image as npimg\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def load_weights(model, resume_model= None):\n",
    "\t'''\n",
    "\t\tLoad saved model state dictionary\n",
    "\t\tInput:\n",
    "\t\t\tresume_model : path of the saved checkpoint to be loaded\n",
    "\t\tOutput:\n",
    "\t\t\tmodel        : weight loaded model\n",
    "\t\t\tstart_epoch  : starting epoch of the training \n",
    "\t'''\n",
    "\tprint(\"=> Loading model weights from {}\".format(resume_model))\n",
    "\tckpt = torch.load(resume_model, map_location=torch.device('cpu'))\n",
    "\n",
    "\tif 'state_dict' in ckpt:\n",
    "\t\tstate_dict = ckpt['state_dict']\n",
    "\telse:\n",
    "\t\tstate_dict = ckpt['model']\n",
    "\t\n",
    "\t# state_dict = ckpt\n",
    "\t\n",
    "\t# state_dict_pretrained = {k.replace('module.', \"\"): v for k,v in state_dict.items()}\n",
    "\tstate_dict_pretrained = state_dict\n",
    "\t# print(model)\n",
    "\t# https://discuss.pytorch.org/t/how-to-load-part-of-pre-trained-model/1113/2\n",
    "\tmodel_dict = model.state_dict()\n",
    "\n",
    "\tstate_dict = {k: v for k, v in state_dict_pretrained.items() if (k in model_dict) and (v.shape == model_dict[k].shape)}\n",
    "\n",
    "\t# overwrite entries in the existing state dict\n",
    "\tmodel_dict.update(state_dict) \n",
    "\n",
    "\t# load the new state dict\n",
    "\tmodel.load_state_dict(model_dict)\n",
    "\n",
    "\tprint(\"Length of model-dict  : {}\".format(len(model_dict)))\t\n",
    "\tprint(\"Length of loaded dict : {}\".format(len(state_dict)))\n",
    "\n",
    "\tif len(model_dict)!=len(state_dict):\n",
    "\t\tnot_in_state_dict = {k: v.shape for k, v in model_dict.items() if k not in state_dict}\n",
    "\t\tprint(\"Layers which are in model-dict but not in loaded state-dict:\")\n",
    "\t\tprint(not_in_state_dict)\n",
    "\n",
    "\t\tnot_in_model_dict = {k: v.shape for k, v in state_dict_pretrained.items() if (k not in state_dict)}\n",
    "\t\tprint(\"Layers which are in loaded state-dict but not in model-dict:\")\n",
    "\t\tprint(not_in_model_dict)\n",
    "\t\n",
    "\tstart_epoch = ckpt['epoch'] + 1\n",
    "\t# start_epoch = 1\n",
    "\treturn model, start_epoch\n",
    "\n",
    "\n",
    "def n_trainable(model):\n",
    "\t# check whether requires_grad is True?\n",
    "\t\n",
    "\ttrainable_parameters = 0\n",
    "\tfor i,j in enumerate(model.parameters(), 1):\n",
    "\t\ttrainable_parameters += j.requires_grad\n",
    "\n",
    "\tif(trainable_parameters == i):\n",
    "\t\tprint(\"All the layers of the model are getting trained\")\n",
    "\telse:\n",
    "\t\tprint(\"Some layers are frozen.\")\n",
    "\t\tprint(\"Number of Layer parameters :{} \\t Trainable layer parameters:{}\".format(i, trainable_parameters))\n",
    "\n",
    "\t# https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\n",
    "\tpytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\tprint(\"Count of trainable-parameters: \", pytorch_total_params)\n",
    "\n",
    "\n",
    "def add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n",
    "\tdecay = []\n",
    "\tno_decay = []\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tif not param.requires_grad:\n",
    "\t\t\tcontinue\n",
    "\t\tif len(param.shape) == 1 or name in skip_list: # bias/bn term or selected layer\n",
    "\t\t\t# print(\"Not applying weight decay to\", name)\n",
    "\t\t\tno_decay.append(param)\n",
    "\t\telse:\n",
    "\t\t\tdecay.append(param)\n",
    "\treturn [\n",
    "\t\t{'params': no_decay, 'weight_decay': 0.},\n",
    "\t\t{'params': decay   , 'weight_decay': weight_decay}]\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, opt, epoch, save_file):\n",
    "\tif isinstance(model, torch.nn.DataParallel):\n",
    "\t\tstate = {\n",
    "\t\t\t'opt': opt,\n",
    "\t\t\t'model': model.cpu().module.state_dict(),\n",
    "\t\t\t'optimizer': optimizer.state_dict(),\n",
    "\t\t\t'epoch': epoch,\n",
    "\t\t}\n",
    "\telse:\n",
    "\t\tstate = {\n",
    "\t\t\t'opt': opt,\n",
    "\t\t\t'model': model.cpu().state_dict(),\n",
    "\t\t\t'optimizer': optimizer.state_dict(),\n",
    "\t\t\t'epoch': epoch,\n",
    "\t\t}\n",
    "\ttorch.save(state, save_file)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "\t\"\"\"\n",
    "\t\tReturns the learning rate for printing\n",
    "\t\"\"\"\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\treturn param_group['lr']\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "\t\"\"\"Computes and stores the average and current value\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.reset()\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.val = 0\n",
    "\t\tself.avg = 0\n",
    "\t\tself.sum = 0\n",
    "\t\tself.count = 0\n",
    "\n",
    "\tdef update(self, val, n=1):\n",
    "\t\tself.val = val\n",
    "\t\tself.sum += val * n\n",
    "\t\tself.count += n\n",
    "\t\tself.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def print_args(args, logger=None):\n",
    "\t\"\"\"\n",
    "\t\tPrint the argument parser\n",
    "\t\"\"\"\n",
    "\tfor k, v in vars(args).items():\n",
    "\t\tif logger is not None:\n",
    "\t\t\tlogger.info('{:<30} : {}'.format(k, v))\n",
    "\t\telse:\n",
    "\t\t\tprint('{:<30} : {}'.format(k, v))\n",
    "\n",
    "\n",
    "def print_header(str):\n",
    "\t\"\"\"\n",
    "\t\tPrint any string with specific format\n",
    "\t\"\"\"\n",
    "\tprint(\"\\n\")\n",
    "\tprint(\"=\"*80)\n",
    "\tprint(\" \"*30, str)\n",
    "\tprint(\"=\"*80)\n",
    "\n",
    "\n",
    "def create_folder(output_folder_path):\n",
    "\t\"\"\"\n",
    "\t\tTries to create a folder if folder is not present\n",
    "\t\"\"\"\n",
    "\tif os.path.exists(output_folder_path):\n",
    "\t\tprint(\"Directory exists {}\".format(output_folder_path))\n",
    "\t\treturn 0\n",
    "\telse:\n",
    "\t\tprint(\"Creating directory {}\".format(output_folder_path))\n",
    "\t\tos.makedirs(output_folder_path)\n",
    "\t\treturn 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
