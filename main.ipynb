{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install pandas\n",
    "# ! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install icecream\n",
    "# ! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install nltk\n",
    "# ! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install plotly\n",
    "# ! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install pytorch_pretrained_bert pytorch-nlp\n",
    "# ! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install keras\n",
    "# ! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install tensorflow\n",
    "# ! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install transformers\n",
    "! /Users/kshitijahande/opt/anaconda3/envs/pytorch/bin/python -m pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rn\n",
    "from tqdm.notebook import tqdm\n",
    "from icecream import ic \n",
    "\n",
    "import nltk\n",
    "# from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# imports from shudima\n",
    "# import torch\n",
    "# from pytorch_pretrained_bert import BertModel\n",
    "# from torch import nn\n",
    "# from torchnlp.datasets import imdb_dataset\n",
    "# from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "# imports from colbert\n",
    "# import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "# import tensorflow.keras.backend as K\n",
    "# from tensorflow import keras\n",
    "# from transformers import *\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# imports from shudima\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# from torch.optim import Adam\n",
    "# from torch.nn.utils import clip_grad_norm_\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# import re\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init \n",
    "train_count = 1000 #8000\n",
    "# test_count = \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 128#colbert=20\n",
    "MAX_SENTENCES = 5\n",
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "# display(list(train_df))\n",
    "display(train_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\", skipinitialspace=True, usecols=['id','text','is_humor'])    #read first 3 columns\n",
    "# test = pd.read_csv(test_loc)\n",
    "train = train[:train_count]\n",
    "\n",
    "# preprocessing section\n",
    "# lower\n",
    "train['text'] = train['text'].str.lower()\n",
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
    "              \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\"]\n",
    "# special char\n",
    "for c in spec_chars:\n",
    "    train['text'] = train['text'].str.replace(c, ' ', regex=True)\n",
    "    \n",
    "#replce double space\n",
    "# train['text'] = train['text'].str.replace('  ', ' ')\n",
    "# train['text'] = re.sub(r\"</?\\[\\d+>\", \"\", train['text'])  \n",
    "train['text'] = train['text'].str.replace('\\s+', ' ', regex=True) \n",
    "\n",
    "# print\n",
    "display(train)\n",
    "display(train.dtypes)\n",
    "\n",
    "# TOKENIZE\n",
    "train['text'] = train['text'].astype(str)\n",
    "# file_dev['Correct Statement']\n",
    "train['word_tokens'] = train.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "# display(train)\n",
    "# file_dev=file_dev.drop(['Correct Statement'], axis = 1)\n",
    "\n",
    "# LEMMA\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_stem_list(row):\n",
    "    stemmed_list = [stemmer.stem(word) for word in row]\n",
    "    return (stemmed_list)\n",
    "\n",
    "train['stem_word_tokens'] = train['word_tokens'].apply(get_stem_list)\n",
    "# display(train['stem_word_tokens'])\n",
    "\n",
    "# STOP WORDS\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def delete_stops(row):\n",
    "    meaningful_words = [w for w in row if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "train['del_stop_word_tokens'] = train['stem_word_tokens'].apply(delete_stops)\n",
    "# train['del_stop_word_tokens'] = train['word_tokens'].apply(delete_stops)\n",
    "\n",
    "display(train)\n",
    "\n",
    "# DROP\n",
    "# train = train['word_tokens'].drop()\n",
    "# train = train['stem_word_tokens'].drop()\n",
    "# file_dev.to_csv('file_dev_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot frequent words for treds.\n",
    "\n",
    "from collections import Counter\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "n=1000\n",
    "\n",
    "# without lemma, and no stop words\n",
    "# sp = train['word_tokens'].apply(delete_stops)\n",
    "# with lemma, no stop words\n",
    "sp = train['del_stop_word_tokens']\n",
    "sim = sp.values.tolist()\n",
    "merged_list = []\n",
    "for l in sim:\n",
    "    merged_list += l\n",
    "freq = Counter(merged_list).most_common(n)\n",
    "# display(freq)\n",
    "\n",
    "# x_axis = list(range(1, n+1)) #rank of words\n",
    "x_axis = list(dict(freq).keys())\n",
    "y_axis = list(dict(freq).values())\n",
    "\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "fig = go.Figure(data = go.Scatter(x= x_axis, y= y_axis))\n",
    "fig.update_layout(title='most frequent words',\n",
    "                   xaxis_title='words',\n",
    "                   yaxis_title='Frequency')\n",
    "# fig.show()\n",
    "pyo.iplot(fig, filename = 'basic-line')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for BERT\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer.tokenize('Hi my nambe is Dima')\n",
    "# train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.utils import *\n",
    "from ipynb.fs.full.datasets import *\n",
    "from ipynb.fs.full.architectures import *\n",
    "from ipynb.fs.full.train_test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled input from the file: data/train.csv\n",
      "   id                                               text  is_humor  \\\n",
      "0   1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
      "1   2  A man inserted an advertisement in the classif...         1   \n",
      "2   3  How many men does it take to open a can of bee...         1   \n",
      "3   4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
      "4   5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
      "\n",
      "   humor_rating  humor_controversy  offense_rating  \n",
      "0          2.42                1.0             0.2  \n",
      "1          2.50                1.0             1.1  \n",
      "2          1.95                0.0             2.4  \n",
      "3          2.11                1.0             0.0  \n",
      "4          2.78                0.0             0.1  \n",
      "Tokenizer: bert-base-uncased\n",
      "\n",
      "Base architecture: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/kshitijahande/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append(\".\")\n",
    "batch_size = 10 #32 or 128\n",
    "\n",
    "\n",
    "traindata = torch.utils.data.DataLoader(dataset= Hahackathon(\"data/train.csv\", basenet= 'bert'), batch_size= batch_size, shuffle= True)\n",
    "# ic(\"Size of the dataset\",len(traindata.dataset))\n",
    "# model = multitask_lstm_fc('bert') #multitask lstm fc is for multitask not single task\n",
    "model = multitask_lstm_fc('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.add_argument('--lr_max',            type = float,          default = 0.00002,      help = 'learning rate (default: 0.01)')\n",
    "learning_rate=0.002 #0.00002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| ce_criterion: CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# ic.enable()\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, f1_score, accuracy_score\n",
    " \n",
    "# mse_criterion = torch.nn.MSELoss()\n",
    "ce_criterion  = torch.nn.CrossEntropyLoss()\n",
    "ic(ce_criterion)\n",
    "loss_arr=[]\n",
    "\n",
    "def concat_output1(output, device):\n",
    "    temp = torch.tensor([]).to(device)\n",
    "\n",
    "    if output.shape[1] == 2:\n",
    "        ic()\n",
    "        temp = torch.argmax(output[:, 0:2], dim=1).view(-1,1)\n",
    "#         ic(temp)\n",
    "        return temp\n",
    "\n",
    "    temp = torch.argmax(output[:, 0:2], dim=1).view(-1,1)\n",
    "    temp = torch.cat((temp, torch.argmax(output[:, 2:4], dim= 1).view(-1,1)), dim= 1)\n",
    "    temp = torch.cat((temp, output[:, 4].view(-1,1)), dim= 1)\n",
    "    temp = torch.cat((temp, output[:, 5].view(-1,1)), dim= 1)\n",
    "    return temp\n",
    "\n",
    "def loss_func1(output, target, device):\n",
    "\n",
    "    humor_id = (target[:, 0]==1)\n",
    "    target   = target.float()\n",
    "#     ic(output[:, 0:2])\n",
    "    \n",
    "    loss1 = ce_criterion(output[:, 0:2], target[:, 0].long())\t                \t# is_humor binary classification\n",
    "#     loss2 = ce_criterion(output[humor_id][:, 2:4], target[humor_id][:, 1].long())\t# humor_controversy binary classification\n",
    "#     loss3 = mse_criterion(output[humor_id][:, 4], target[humor_id][:, 2])\t\t\t# humor rating regression loss\n",
    "#     loss4 = mse_criterion(output[:, 5], target[:, 3])\t\t\t\t\t\t\t\t# offensive rating regression loss\n",
    "#     loss  = loss1+loss2+loss3+loss4\n",
    "    loss=loss1\n",
    "    \n",
    "    # this block of generating temp is verified throughly\n",
    "    temp = concat_output1(output, device)\n",
    "#     ic('outside concat_output ', temp)\n",
    "    temp = temp.detach()\n",
    "\n",
    "    return loss, temp\n",
    "\n",
    "\n",
    "def train1(model, dataloader, optimizer, device):\n",
    "#     ic('1')\n",
    "    model.train()\n",
    "#     ic('2')\n",
    "    train_loss = AverageMeter()\n",
    "\n",
    "    for batch_id, batch_data in enumerate(dataloader):\n",
    "#         ic(batch_data[0], batch_data[1])\n",
    "        data       = batch_data[0]\n",
    "        target     = batch_data[1].to(device)\n",
    "#         ic(data)\n",
    "#         ic(target)\n",
    "\n",
    "        input_id   = data[0]\n",
    "        mask_id    = data[1]\n",
    "        segment_id = data[2]\n",
    "#         ic(input_id)\n",
    "#         ic(mask_id)\n",
    "#         ic(segment_id)\n",
    "        \n",
    "        output = model(input_id, mask_id, segment_id)\n",
    "#         ic('output model')\n",
    "        \n",
    "        loss, _ = loss_func1(output, target, device= device)\n",
    "        ic('outside loss_func')\n",
    "        loss_arr.append(loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.update(loss.item(), target.shape[0])\n",
    "\n",
    "    return train_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-76c007411845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# ic(optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-2ae2a933f43d>\u001b[0m in \u001b[0;36mtrain1\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#         ic(segment_id)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;31m#         ic('output model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/NLP/humour-detection-project/architectures.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_id, mask_id, token_type_id, go_input_id, go_mask_id)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;34m\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t torch.nn.Dropout(p= dropout_prob))\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;34m\"\\t\\tself.classifier2_2 = torch.nn.Sequential(torch.nn.Linear(in_features= fc_dim, out_features= 2))\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;34m\"\\t\\t\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;34m\"\\tdef forward(self, input_id, mask_id, token_type_id, go_input_id= None, go_mask_id= None):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspace/NLP/humour-detection-project/architectures.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_id, mask_id, token_type_id)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m\"\\t\\t# feat.keys() = ['last_hidden_state', 'pooler_output']\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;34m\"\\t\\tif self.temporal:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;34m\"\\t\\t\\tif self.basenet == 'roberta':\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m\"\\t\\t\\t\\tfeat = self.encoder(input_ids= input_id, attention_mask= mask_id)['last_hidden_state']\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "use_cuda= False\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# ic(traindata)\n",
    "# ic(optimizer)\n",
    "\n",
    "train_loss = train1(model, traindata, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from IITK code\n",
    "# # note - DO NOT USE STOP WORDS\n",
    "# # function from datasets.py\n",
    "# def get_tokenized_text(self, text):\n",
    "#     # marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "# #     max_length= 128, stop_words= False, basenet=bert, return_tensors= nope\n",
    "#     train_encoded_inputs = tokenizer(text= text, # the sentence to be encoded\n",
    "#                     add_special_tokens= True,  # add [CLS] and [SEP]\n",
    "#                     max_length= MAX_SENTENCE_LENGTH  # maximum length of a sentence\n",
    "#                     padding= 'max_length',  # add [PAD]s\n",
    "#                     return_attention_mask= True,  # generate the attention mask\n",
    "# #                     return_tensors = 'pt',  # return PyTorch tensors\n",
    "#                     truncation= True) \n",
    "\n",
    "#     input_id = encoded['input_ids']\n",
    "#     mask_id  = encoded['attention_mask']\n",
    "# # input_segments= something related to tensors??? - it is sentence id to distinguish between sentences! - IMP \n",
    "# # , label= id, text, is _humor ??\n",
    "#     segment_id = [1] * MAX_SENTENCE_LENGTH\n",
    "\n",
    "#     return input_id, mask_id, segment_id\n",
    "\n",
    "\n",
    "# def compute_input_arrays(df, columns, tokenizer):\n",
    "#     model_input = []\n",
    "# #     for xx in range((MAX_SENTENCES*3)+3):\n",
    "# #         model_input.append([])\n",
    "    \n",
    "#     for _, row in tqdm(df[columns].iterrows()):\n",
    "#         i = 0\n",
    "        \n",
    "#         # sent\n",
    "#     ic(model_input[0].shape)\n",
    "#     return model_input\n",
    "\n",
    "# # train_encoded_inputs = compute_input_arrays(train, list(train.columns[[1]]), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from colbert code\n",
    "# def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "# #     ic(str1)\n",
    "# #     ic(str2)\n",
    "#     inputs = tokenizer.encode_plus(str1, str2,\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=length,\n",
    "#         truncation_strategy=truncation_strategy)\n",
    "\n",
    "# #     ic(inputs)\n",
    "# #     ic(inputs[\"input_ids\"])\n",
    "#     input_ids =  inputs[\"input_ids\"]\n",
    "#     input_masks = [1] * len(input_ids)\n",
    "#     input_segments = inputs[\"token_type_ids\"]\n",
    "#     padding_length = length - len(input_ids)\n",
    "#     padding_id = tokenizer.pad_token_id\n",
    "#     input_ids = input_ids + ([padding_id] * padding_length)\n",
    "#     input_masks = input_masks + ([0] * padding_length)\n",
    "#     input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "#     return [input_ids, input_masks, input_segments]\n",
    "\n",
    "\n",
    "# def compute_input_arrays(df, columns, tokenizer):\n",
    "#     model_input = []\n",
    "#     for xx in range((MAX_SENTENCES*3)+3):\n",
    "#         model_input.append([])\n",
    "    \n",
    "#     for _, row in tqdm(df[columns].iterrows()):\n",
    "#         i = 0\n",
    "        \n",
    "#         # sent\n",
    "# #         ic(row)\n",
    "# #         ic(row.text)\n",
    "#         sentences = sent_tokenize(row.text)\n",
    "#         for xx in range(MAX_SENTENCES):\n",
    "#             s = sentences[xx] if xx<len(sentences) else ''\n",
    "# #             ic(xx)\n",
    "#             ids_q, masks_q, segments_q = return_id(s, None, 'longest_first', MAX_SENTENCE_LENGTH)\n",
    "#             model_input[i].append(ids_q)\n",
    "#             i+=1\n",
    "#             model_input[i].append(masks_q)\n",
    "#             i+=1\n",
    "#             model_input[i].append(segments_q)\n",
    "#             i+=1\n",
    "        \n",
    "#         # full row\n",
    "#         ids_q, masks_q, segments_q = return_id(row.text, None, 'longest_first', MAX_LENGTH)\n",
    "#         model_input[i].append(ids_q)\n",
    "#         i+=1\n",
    "#         model_input[i].append(masks_q)\n",
    "#         i+=1\n",
    "#         model_input[i].append(segments_q)\n",
    "        \n",
    "#     for xx in range((MAX_SENTENCES*3)+3):\n",
    "#         model_input[xx] = np.asarray(model_input[xx], dtype=np.int32)\n",
    "#     ic('model input is train examples x bert features')\n",
    "#     ic(model_input[0].shape)\n",
    "#     return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(list(train.columns[[1]]))\n",
    "# # train_encoded_inputs\n",
    "# train_encoded_inputs = compute_input_arrays(train, list(train.columns[[1]]), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic('model inputs, rows, colums/bert features')\n",
    "# ic(len(train_encoded_inputs), len(train_encoded_inputs[0]), len(train_encoded_inputs[0][0]))\n",
    "\n",
    "# # check out input for 7th row\n",
    "# r = 7\n",
    "# ic(train.iloc[r])\n",
    "# ic(train.iloc[r,1])\n",
    "# ic(sent_tokenize(train.iloc[r,1]))\n",
    "# # en_text[0][xx], en_text[3][xx], en_text[6][xx], en_text[15][xx]\n",
    "# train_encoded_inputs[0][r], train_encoded_inputs[3][r], train_encoded_inputs[6][r], train_encoded_inputs[15][r]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
