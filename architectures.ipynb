{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a306d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************************************************************\n",
    "# \n",
    "# Contains different multitask architectures used for training the model\n",
    "# \n",
    "# temporal   : Embeddings followed by a LSTM layer\n",
    "# \n",
    "# ******************************************************************\n",
    "\n",
    "# Dropout is placed after (fc output + ReLU)\n",
    "# https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "# No activation after LSTM layer\n",
    "# https://towardsdatascience.com/reading-between-the-layers-lstm-network-7956ad192e58\n",
    "\n",
    "# Using dropout in the LSTM layer\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from transformers import BertModel, AutoModel, RobertaModel, DebertaForSequenceClassification, DebertaModel\n",
    "\n",
    "dropout_prob= 0.2\n",
    "\n",
    "class BERT_based_classifier(torch.nn.Module):\n",
    "\t'''\n",
    "\t\tBase class inherited by different architectures used for multitask learning\n",
    "\t'''\n",
    "\t\n",
    "\tdef __init__(self, basenet= 'bert', bert_freeze= False, temporal= False, n_outputs= 2, penultimate= False, fc_dim= 256, lstm_dim= 256, one_layer= False, no_classfier= False):      \n",
    "\t\t\n",
    "\t\tsuper(BERT_based_classifier, self).__init__()\n",
    "\n",
    "\t\t# Load pre-trained model (weights)\n",
    "\t\tself.basenet = basenet\n",
    "\t\tif basenet == 'bert':\n",
    "\t\t\tprint(\"Base architecture: bert-base-uncased\")\n",
    "\t\t\tself.encoder = BertModel.from_pretrained('bert-base-uncased',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "# \t\telif basenet == 'ernie':\n",
    "# \t\t\tprint(\"Base architecture: ernie-2.0-en\")\n",
    "# \t\t\tself.encoder = AutoModel.from_pretrained(\"nghuyong/ernie-2.0-en\",\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t  output_hidden_states = False\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "# \t\telif basenet == 'roberta':\n",
    "# \t\t\tprint(\"Base architecture: roberta-base\")\n",
    "# \t\t\tself.encoder = RobertaModel.from_pretrained('roberta-base',\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t output_hidden_states = False\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t   )\n",
    "# \t\telif basenet == 'deberta':\n",
    "# \t\t\tprint(\"Base architecture: microsoft/deberta-base\")\n",
    "# \t\t\tself.encoder = DebertaModel.from_pretrained('microsoft/deberta-base',\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t output_hidden_states = False\n",
    "# \t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\n",
    "# \t\tif bert_freeze:\n",
    "# \t\t\tprint(\"Freezing BERT!\")\n",
    "# \t\t\t# freeze bert so that is is not finetuned\n",
    "# \t\t\tfor name, param in self.encoder.named_parameters():                \n",
    "# \t\t\t\tif param.requires_grad is not None:\n",
    "# \t\t\t\t\tparam.requires_grad = False\n",
    "\n",
    "\t\tself.temporal = temporal\n",
    "\t\tif self.temporal:\n",
    "\t\t\tself.lstm       = torch.nn.LSTM(input_size= 768, hidden_size= lstm_dim, dropout= dropout_prob)\n",
    "\t\t\tself.classifier = torch.nn.Sequential(torch.nn.Linear(in_features= fc_dim, out_features= n_outputs))\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif not one_layer:\n",
    "\t\t\t\tself.classifier = torch.nn.Sequential(torch.nn.Linear(in_features= 768, out_features= fc_dim),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  torch.nn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  torch.nn.Dropout(p= dropout_prob),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  torch.nn.Linear(in_features= fc_dim, out_features= n_outputs))\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.classifier = torch.nn.Sequential(torch.nn.Linear(in_features= 768, out_features= n_outputs))\n",
    "\n",
    "\t\t\n",
    "\tdef forward(self, input_id, mask_id, token_type_id):\n",
    "\t\t\n",
    "\t\t# if output_hidden_states == True:\n",
    "\t\t# feat.keys() = ['last_hidden_state', 'hidden_states']\n",
    "\t\t# if output_hidden_states == False:\n",
    "\t\t# feat.keys() = ['last_hidden_state', 'pooler_output']\n",
    "\n",
    "\t\tif self.temporal:\n",
    "\t\t\tif self.basenet == 'roberta':\n",
    "\t\t\t\tfeat = self.encoder(input_ids= input_id, attention_mask= mask_id)['last_hidden_state']\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeat = self.encoder(input_ids= input_id, attention_mask= mask_id, token_type_ids= token_type_id)['last_hidden_state']\n",
    "\n",
    "\t\t\tfeat    = feat.squeeze().permute(1, 0, 2)\n",
    "\t\t\t_, temp = self.lstm(feat)\n",
    "\t\t\tfeat    = temp[0].squeeze()\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif self.basenet == 'roberta':\n",
    "\t\t\t\tfeat = self.encoder(input_ids= input_id, attention_mask= mask_id)['pooler_output']\n",
    "\t\t\telse:\n",
    "\t\t\t\tfeat = self.encoder(input_ids= input_id, attention_mask= mask_id, token_type_ids= token_type_id)['pooler_output']\n",
    "\n",
    "\t\treturn feat\n",
    "\n",
    "\n",
    "class multitask_fc(BERT_based_classifier):\n",
    "\t'''\n",
    "\tLLM embeddings followed by one/two FC layer with MCE/CE loss\n",
    "\t\n",
    "\tif one_layer:\n",
    "\t\tbasenet -> FC -> MSE/CE\n",
    "\telse:\n",
    "\t\tbasenet -> FC   -> ReLU -> Dropout -> FC -> MSE + CE\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, temporal= False, n_outputs= 6, fc_dim= 256, one_layer= False):      \n",
    "\t\t\n",
    "\t\tsuper(multitask_fc, self).__init__(temporal= temporal, n_outputs= n_outputs, fc_dim= fc_dim, one_layer= one_layer)\n",
    "\t\t\n",
    "\tdef forward(self, input_id, mask_id, token_type_id, go_input_id= None, go_mask_id= None):\n",
    "\t\t\n",
    "\t\tfeat   = super(multitask_fc, self).forward(input_id, mask_id, token_type_id)\t\t#torch.Size([64, 256])\t\t\n",
    "\t\toutput = self.classifier(feat)\n",
    "\t\treturn output\n",
    "\n",
    "\n",
    "class multitask_lstm_fc(BERT_based_classifier):\n",
    "\t'''\n",
    "\t  Mutitask model with two separate branches for classification and regression task\n",
    "\n",
    "\t  temporal= True\n",
    "\t  basenet -> LSTM -> FC   -> CE\n",
    "\t\t  |\n",
    "\t\t   ----> FC   -> ReLU -> Dropout -> FC -> MSE\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, basenet= 'bert', temporal= True, bert_freeze= False, fc_dim= 256, in_features= 768):      \n",
    "\n",
    "\t\tsuper(multitask_lstm_fc, self).__init__(basenet= basenet, temporal= temporal, bert_freeze= False, n_outputs= 4, fc_dim= fc_dim)\n",
    "\t\tself.classifier2_1 = torch.nn.Sequential(torch.nn.Linear(in_features= 768, out_features= 256),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t torch.nn.ReLU(),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t torch.nn.Dropout(p= dropout_prob))\n",
    "\n",
    "\t\tself.classifier2_2 = torch.nn.Sequential(torch.nn.Linear(in_features= fc_dim, out_features= 2))\n",
    "\t\t\n",
    "\tdef forward(self, input_id, mask_id, token_type_id, go_input_id= None, go_mask_id= None):\n",
    "\t\t\n",
    "\t\tfeat1   = super(multitask_lstm_fc, self).forward(input_id, mask_id, token_type_id)\t\t#torch.Size([64, 256])\n",
    "\n",
    "\t\tif self.basenet == 'roberta':\n",
    "\t\t\tfeat2 = self.classifier2_1(self.encoder(input_ids= input_id, attention_mask= mask_id)['pooler_output'])\n",
    "\t\telse:\n",
    "\t\t\tfeat2 = self.classifier2_1(self.encoder(input_ids= input_id, attention_mask= mask_id, token_type_ids= token_type_id)['pooler_output'])\n",
    "\n",
    "\t\toutput1 = self.classifier (feat1)\t\t\n",
    "\t\toutput2 = self.classifier2_2(feat2)\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "\t\t# torch.Size([128, 4]) torch.Size([128, 2]) torch.Size([128, 6])\n",
    "\t\toutput  = torch.cat((output1, output2), dim= 1)\n",
    "\n",
    "\t\treturn output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
