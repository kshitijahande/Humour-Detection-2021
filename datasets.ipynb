{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"pytorch","language":"python","name":"pytorch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"datasets.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0nsbNUgUafX","executionInfo":{"status":"ok","timestamp":1625926574698,"user_tz":-330,"elapsed":13028,"user":{"displayName":"Kshitija Hande","photoUrl":"","userId":"02509873836572328998"}},"outputId":"aade3d50-fd38-4d6d-e098-e37a10fd47da"},"source":["!pip install -qq icecream\n","!pip install -qq transformers"],"id":"h0nsbNUgUafX","execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting icecream\n","  Downloading https://files.pythonhosted.org/packages/1f/c0/8e2bc1b5eab95e5155841c826b431692638c19bf04ee4cdc86b379f85150/icecream-2.1.1-py2.py3-none-any.whl\n","Collecting asttokens>=2.0.1\n","  Downloading https://files.pythonhosted.org/packages/16/d5/b0ad240c22bba2f4591693b0ca43aae94fbd77fb1e2b107d54fff1462b6f/asttokens-2.0.5-py2.py3-none-any.whl\n","Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from icecream) (2.6.1)\n","Collecting executing>=0.3.1\n","  Downloading https://files.pythonhosted.org/packages/17/85/b84ea78f52bcb5513a790e64edc19687d8699ea6b4197f075da28547a370/executing-0.7.0-py2.py3-none-any.whl\n","Collecting colorama>=0.3.9\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream) (1.15.0)\n","Installing collected packages: asttokens, executing, colorama, icecream\n","Successfully installed asttokens-2.0.5 colorama-0.4.4 executing-0.7.0 icecream-2.1.1\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 3.9MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 20.3MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 40.5MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"37d2e59e"},"source":["# *****************************************\n","# \n","# Customised dataset class for \n","# SemEval 2021 Task 7: Hahackathon\n","# \n","# ****************************************\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from icecream import ic \n","\n","from transformers import AutoTokenizer, BertTokenizer, RobertaTokenizer, DebertaTokenizer\n","\n","import sys\n","\n","class Hahackathon(torch.utils.data.Dataset):\n","\t'''\n","\tHahackathon dataset\n","\n","\tfilename: train/val/test file to be read\n","\n","\tbasenet : bert/ernie/roberta/deberta\n","\n","\tis_test : if the input file does not have groundtruth labels\n","\t        : (for evaluation on the leaderboard)\n","\t'''\n","\n","\tdef __init__(self, filename, basenet= 'bert', max_length= 128, stop_words= False, is_test= False):\n","            super(Hahackathon, self).__init__()\n","\n","            self.is_test = is_test\n","\n","            if stop_words:\n","                self.nlp  = English()\n","\n","            self.data    = self.read_file(filename, stop_words)\n","\n","            if basenet == 'bert':\n","                print(\"Tokenizer: bert-base-uncased\\n\")\n","                self.token = BertTokenizer.from_pretrained('bert-base-uncased')\n","            elif basenet == 'ernie':\n","                print(\"Tokenizer: ernie-2.0-en\\n\")\n","                self.token = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n","            elif basenet == 'roberta':\n","                print(\"Tokenizer: roberta-base\\n\")\n","                self.token = RobertaTokenizer.from_pretrained('roberta-base')\n","            elif basenet == 'deberta':\n","                print(\"Tokenizer: microsoft/deberta-base\\n\")\n","                self.token = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n","            self.max_length = max_length\n","            self.segment_id = torch.tensor([1] * self.max_length).view(1, -1)\n","\n","\t\t\n","\tdef read_file(self, filename, stop_words):\n","\t\tdf = pd.read_csv(filename)\n","\n","\t\t# removing stop-words\n","\t\t# https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n","\t\tif stop_words:\n","\t\t\tfor i in range(len(df)):\n","\t\t\t\ttext = df.iloc[i]['text']\n","\t\t\t\ttext = text.split(' ')\n","\t\t\t\tfiltered_text =[] \n","\n","\t\t\t\tfor word in text:\n","\t\t\t\t\tlexeme = self.nlp.vocab[word]\n","\t\t\t\t\tif lexeme.is_stop == False:\n","\t\t\t\t\t\tfiltered_text.append(word) \n","\t\t\t\t# print(\"original\", text)\n","\t\t\t\ttext = ' '.join(filtered_text)\n","\t\t\t\t# print(\"after\", filtered_text)\n","\t\t\t\t# print(\"after\", text)\n","\t\t\t\tdf.loc[i, 'text'] = text\n","\n","\t\t# replace all NaN with 0\n","\t\t# will be used dring loss function computation\n","\t\tdf = df.fillna(0)\n","\n","# \t\tif not self.is_test:\n","# \t\t\tdf['humor_controversy'] = df['humor_controversy'].astype('int')\n","\n","\t\t# print(df.shape)\n","\t\tprint(\"Sampled input from the file: {}\".format(filename))\n","\t\tprint(df.head())\n","\n","\t\treturn df\n","\n","\t\n","\tdef get_tokenized_text(self, text):\t\t\n","\t\t# marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\t\tencoded = self.token(text= text,  \t\t\t\t\t# the sentence to be encoded\n","\t\t\t\t\t\t\t add_special_tokens= True,  \t# add [CLS] and [SEP]\n","\t\t\t\t\t\t\t max_length= self.max_length,  \t# maximum length of a sentence\n","\t\t\t\t\t\t\t padding= 'max_length',  \t\t# add [PAD]s\n","\t\t\t\t\t\t\t return_attention_mask= True,  \t# generate the attention mask\n","\t\t\t\t\t\t\t return_tensors = 'pt',  \t\t# return PyTorch tensors\n","\t\t\t\t\t\t\t truncation= True\n","\t\t\t\t\t\t\t) \n","\n","\t\tinput_id = encoded['input_ids']\n","\t\tmask_id  = encoded['attention_mask']\n","\n","\t\treturn input_id, mask_id\n","\n","\t\t\n","\tdef __len__(self):\n","\t\treturn len(self.data)\n","\t\n","\n","\tdef __getitem__(self, idx):\n","            text  = self.data.iloc[idx]['text']\n","            label = []\n","            if not self.is_test:\n","                label.append(self.data.iloc[idx]['is_humor'])\n","            #             label.append(self.data.iloc[idx]['humor_controversy'])\n","            #             label.append(self.data.iloc[idx]['humor_rating'])\n","            #             label.append(self.data.iloc[idx]['offense_rating'])\n","\n","            else:\n","                label.append(self.data.iloc[idx]['id'])\n","\n","            label = torch.tensor(label)\n","\n","            input_id, mask_id  = self.get_tokenized_text(text)\n","\n","            return [input_id, mask_id, self.segment_id], label\n","\n","\n"],"id":"37d2e59e","execution_count":null,"outputs":[]}]}