{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37d2e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************\n",
    "# \n",
    "# Customised dataset class for \n",
    "# SemEval 2021 Task 7: Hahackathon\n",
    "# \n",
    "# ****************************************\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from icecream import ic \n",
    "\n",
    "from transformers import AutoTokenizer, BertTokenizer, RobertaTokenizer, DebertaTokenizer\n",
    "\n",
    "import sys\n",
    "\n",
    "class Hahackathon(torch.utils.data.Dataset):\n",
    "\t'''\n",
    "\tHahackathon dataset\n",
    "\n",
    "\tfilename: train/val/test file to be read\n",
    "\n",
    "\tbasenet : bert/ernie/roberta/deberta\n",
    "\n",
    "\tis_test : if the input file does not have groundtruth labels\n",
    "\t        : (for evaluation on the leaderboard)\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, filename, basenet= 'bert', max_length= 128, stop_words= False, is_test= False):\n",
    "            super(Hahackathon, self).__init__()\n",
    "\n",
    "            self.is_test = is_test\n",
    "\n",
    "            if stop_words:\n",
    "                self.nlp  = English()\n",
    "\n",
    "            self.data    = self.read_file(filename, stop_words)\n",
    "\n",
    "            if basenet == 'bert':\n",
    "                print(\"Tokenizer: bert-base-uncased\\n\")\n",
    "                self.token = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#             elif basenet == 'ernie':\n",
    "#                 print(\"Tokenizer: ernie-2.0-en\\n\")\n",
    "#                 self.token = AutoTokenizer.from_pretrained(\"nghuyong/ernie-2.0-en\")\n",
    "#             elif basenet == 'roberta':\n",
    "#                 print(\"Tokenizer: roberta-base\\n\")\n",
    "#                 self.token = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#             elif basenet == 'deberta':\n",
    "#                 print(\"Tokenizer: microsoft/deberta-base\\n\")\n",
    "#                 self.token = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "            self.max_length = max_length\n",
    "            self.segment_id = torch.tensor([1] * self.max_length).view(1, -1)\n",
    "\n",
    "#             self.segment_id = [1] * self.max_length\n",
    "\n",
    "\t\t\n",
    "\tdef read_file(self, filename, stop_words):\n",
    "\t\tdf = pd.read_csv(filename)\n",
    "\n",
    "\t\t# removing stop-words\n",
    "\t\t# https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n",
    "# \t\tif stop_words:\n",
    "# \t\t\tfor i in range(len(df)):\n",
    "# \t\t\t\ttext = df.iloc[i]['text']\n",
    "# \t\t\t\ttext = text.split(' ')\n",
    "# \t\t\t\tfiltered_text =[] \n",
    "\n",
    "# \t\t\t\tfor word in text:\n",
    "# \t\t\t\t\tlexeme = self.nlp.vocab[word]\n",
    "# \t\t\t\t\tif lexeme.is_stop == False:\n",
    "# \t\t\t\t\t\tfiltered_text.append(word) \n",
    "# \t\t\t\t# print(\"original\", text)\n",
    "# \t\t\t\ttext = ' '.join(filtered_text)\n",
    "# \t\t\t\t# print(\"after\", filtered_text)\n",
    "# \t\t\t\t# print(\"after\", text)\n",
    "# \t\t\t\tdf.loc[i, 'text'] = text\n",
    "\n",
    "\t\t# replace all NaN with 0\n",
    "\t\t# will be used dring loss function computation\n",
    "\t\tdf = df.fillna(0)\n",
    "\n",
    "# \t\tif not self.is_test:\n",
    "# \t\t\tdf['humor_controversy'] = df['humor_controversy'].astype('int')\n",
    "\n",
    "\t\t# print(df.shape)\n",
    "\t\tprint(\"Sampled input from the file: {}\".format(filename))\n",
    "\t\tprint(df.head())\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\t\n",
    "\tdef get_tokenized_text(self, text):\t\t\n",
    "\t\t# marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\t\tencoded = self.token(text= text,  \t\t\t\t\t# the sentence to be encoded\n",
    "\t\t\t\t\t\t\t add_special_tokens= True,  \t# add [CLS] and [SEP]\n",
    "\t\t\t\t\t\t\t max_length= self.max_length,  \t# maximum length of a sentence\n",
    "\t\t\t\t\t\t\t padding= 'max_length',  \t\t# add [PAD]s\n",
    "\t\t\t\t\t\t\t return_attention_mask= True,  \t# generate the attention mask\n",
    "\t\t\t\t\t\t\t return_tensors = 'pt',  \t\t# return PyTorch tensors\n",
    "\t\t\t\t\t\t\t truncation= True\n",
    "\t\t\t\t\t\t\t) \n",
    "\n",
    "\t\tinput_id = encoded['input_ids']\n",
    "\t\tmask_id  = encoded['attention_mask']\n",
    "\n",
    "\t\treturn input_id, mask_id\n",
    "\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\t\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "            text  = self.data.iloc[idx]['text']\n",
    "            label = []\n",
    "            if not self.is_test:\n",
    "                label.append(self.data.iloc[idx]['is_humor'])\n",
    "            #             label.append(self.data.iloc[idx]['humor_controversy'])\n",
    "            #             label.append(self.data.iloc[idx]['humor_rating'])\n",
    "            #             label.append(self.data.iloc[idx]['offense_rating'])\n",
    "\n",
    "            else:\n",
    "                label.append(self.data.iloc[idx]['id'])\n",
    "\n",
    "            label = torch.tensor(label)\n",
    "\n",
    "            input_id, mask_id  = self.get_tokenized_text(text)\n",
    "\n",
    "            return [input_id, mask_id, self.segment_id], label\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
